{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN进行情绪分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们将使用RNN(循环神经网络)进行情感分析，至于为什么使用RNN而不是普通的前馈神经网络，是因为RNN能够存储序列单词信息，得到的结果更为准确。这里我们将使用一个带有标签的影评数据集进行训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用的RNN模型架构如下：\n",
    "<img src=\"assets/network_diagram.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们将单词传入到嵌入层而不是使用ONE-HOT编码，是因为词嵌入是一种对单词数据更好的表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在嵌入层之后，新的表示将会进入LSTM细胞层。最后使用一个全连接层作为输出层。我们使用sigmiod作为激活函数，因为我们的结果只有positive和negative两个表示情感的结果。输出层将是一个使用sigmoid作为激活函数的单一的单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sentiment-network/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('../sentiment-network/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33678267\n",
      "225000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\nstory of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \\nhomelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建神经网络的第一步是将数据处理成合适的格式，由于我们需要将数据输入到嵌入层，因此需要将每一个单词\n",
    "编码为整数形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据集中，每条评论是用换行符分隔的。为了解决这些问题，我将把文本分成每一个评论，使用\\n作为分隔符。然后我可以把所有的评论组合成一个大的字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们将移除数据中所有的标点符号，然后去掉所有的换行符，得到所有单独的单词组成的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "\n",
    "all_text = ''.join(reviews)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   story of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers  unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting  even those from the era should be turned off  the cryptic dialogue would make shakespeare seem easy to a third grader  on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond  future stars sally kirkland and frederic forrest can be seen briefly   homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter  most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets   br    br   but what if you were given a bet to live on the stre'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对单词进行编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嵌入查找要求传入整数到网络中，最简单的方法是创建一个从单词到整数的映射的字典。然后我们能将每条评论转换为整数传入网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6020196\n",
      "74072\n",
      "25001\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(set(words)))\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n",
      "74072\n"
     ]
    }
   ],
   "source": [
    "set_words = set(words)\n",
    "print(len(set_words))\n",
    "list_words = list(set_words)\n",
    "print(len(list_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word : ii for ii, word in enumerate(vocab, 1)}\n",
    "reviews_ints = []\n",
    "\n",
    "for review in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25001\n",
      "[63, 4, 3, 125, 36, 47, 7472, 1395, 16, 3, 4181, 505, 45, 17, 3, 622, 134, 12, 6, 3, 1279, 457, 4, 1721, 207, 3, 10624, 7373, 300, 6, 667, 83, 35, 2116, 1086, 2989, 34, 1, 898, 46417, 4, 8, 13, 5096, 464, 8, 2656, 1721, 1, 221, 57, 17, 58, 794, 1297, 832, 228, 8, 43, 98, 123, 1469, 59, 147, 38, 1, 963, 142, 29, 667, 123, 1, 13584, 410, 61, 94, 1774, 306, 755, 5, 3, 819, 10396, 22, 3, 1724, 635, 8, 13, 128, 73, 21, 233, 102, 17, 49, 50, 617, 34, 682, 85, 28785, 28786, 682, 374, 3341, 11398, 2, 16371, 7946, 51, 29, 108, 3324]\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_ints))\n",
    "print(reviews_ints[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对标签进行编码\n",
    "我们的标签有'positive'和'negative'两种，为了在网络中使用它们，我们需要将两个标签转换为1和0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([0 if label == 'negative' else 0 for label in labels.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print('Zero-length reviews: {}'.format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面我们发现有一条评论的长度为0，另一方面，有的评论长度太长，对于RNN训练来说，需要太多的步骤，因此我们的处理方法是将每条评论的单词数控制为200个单词。这意味着对于不足200个单词的评论，将用0补上，对于超过200个单词的评论，我们只截取前200个使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25001\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "#移除长度为0的评论\n",
    "print(len(reviews_ints))\n",
    "reviews_ints = [review_int for review_int in reviews_ints if len(review_int) > 0]\n",
    "print(len(reviews_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们需要创建一个用于存储输入网络的数据的矩阵。数据来源于reviews_ints，因为我们需要传入数字到网络中，并且每行代表一条评论，长度都是200，对于长度短于200的评论，使用0填充，例如，这是其中一条评论['best', 'movie', ever'],对应的编码是[11,23,354],处理后的行应该是这样：[0,0,0......,11,23,354].对于长度大于200的评论，使用前200个单词作为特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "#处理多余200个单词的评论\n",
    "reviews_ints = [review[:200] for review in reviews_ints]\n",
    "#处理少于200个单词的评论\n",
    "features = []\n",
    "for review in reviews_ints:\n",
    "    if len(review) < seq_len : \n",
    "        s = []\n",
    "        for i in range(seq_len - len(review)):\n",
    "            s.append(0)\n",
    "        s.extend(review)\n",
    "        features.append(s)\n",
    "    else:\n",
    "        features.append(review)\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 21025,   308,     6,\n",
       "            3,  1050,   207,     8,  2138,    32,     1,   171,    57,\n",
       "           15,    49,    81,  5785,    44,   382,   110,   140,    15,\n",
       "         5194,    60,   154,     9,     1,  4975,  5852,   475,    71,\n",
       "            5,   260,    12, 21025,   308,    13,  1978,     6,    74,\n",
       "         2395],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,    63,     4,     3,   125,\n",
       "           36,    47,  7472,  1395,    16,     3,  4181,   505,    45,\n",
       "           17],\n",
       "       [22382,    42, 46418,    15,   706, 17139,  3389,    47,    77,\n",
       "           35,  1819,    16,   154,    19,   114,     3,  1305,     5,\n",
       "          336,   147,    22,     1,   857,    12,    70,   281,  1168,\n",
       "          399,    36,   120,   283,    38,   169,     5,   382,   158,\n",
       "           42,  2269,    16,     1,   541,    90,    78,   102,     4,\n",
       "            1,  3244,    15,    43,     3,   407,  1068,   136,  8055,\n",
       "           44,   182,   140,    15,  3043,     1,   320,    22,  4818,\n",
       "        26224,   346,     5,  3090,  2092,     1, 18839, 17939,    42,\n",
       "         8055,    46,    33,   236,    29,   370,     5,   130,    56,\n",
       "           22,     1,  1928,     7,     7,    19,    48,    46,    21,\n",
       "           70,   344,     3,  2099,     5,   408,    22,     1,  1928,\n",
       "           16],\n",
       "       [ 4505,   505,    15,     3,  3342,   162,  8312,  1652,     6,\n",
       "         4819,    56,    17,  4504,  5616,   140, 11725,     5,   996,\n",
       "         4919,  2933,  4462,   566,  1201,    36,     6,  1518,    96,\n",
       "            3,   744,     4, 26225,    13,     5,    27,  3461,     9,\n",
       "        10625,     4,     8,   111,  3013,     5,     1,  1027,    15,\n",
       "            3,  4390,    82,    22,  2049,     6,  4462,   538,  2764,\n",
       "         7073, 37443,    41,   463,     1,  8312, 46419,   302,   123,\n",
       "           15,  4221,    19,  1667,   922,     1,  1652,     6,  6129,\n",
       "        19871,    34,     1,   980,  1751, 22383,   646, 24104,    27,\n",
       "          106, 11726,    13, 14045, 15097, 17940,  2457,   466, 21027,\n",
       "           36,  3266,     1,  6365,  1020,    45,    17,  2695,  2499,\n",
       "           33],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,   520,   119,   113,    34,\n",
       "        16372,  1816,  3737,   117,   885, 21030,   721,    10,    28,\n",
       "          124,   108,     2,   115,   137,     9,  1623,  7691,    26,\n",
       "          330,     5,   589,     1,  6130,    22,   386,     6,     3,\n",
       "          349,    15,    50,    15,   231,     9,  7473, 11399,     1,\n",
       "          191,    22,  8966,     6,    82,   880,   101,   111,  3584,\n",
       "            4],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           11,    20,  3637,   141,    10,   422,    23,   272,    60,\n",
       "         4355,    22,    32,    84,  3286,    22,     1,   172,     4,\n",
       "            1,   952,   507,    11,  4977,  5361,     5,   574,     4,\n",
       "         1155,    54,    53,  5304,     1,   261,    17,    41,   952,\n",
       "          125,    59,     1,   711,   137,   379,   626,    15,   111,\n",
       "         1509],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,    11,     6,   692,     1,    90,\n",
       "         2156,    20, 11728,     1,  2818,  5195,   249,    92,  3006,\n",
       "            8,   126,    24,   200,     3,   802,   634,     4, 22382,\n",
       "         1001],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,   786,   295,    10,   122,    11,     6,   419,\n",
       "            5,    29,    35,   482,    20,    19,  1281,    33,   142,\n",
       "           28,  2657,    45,  1840,    32,     1,  2778,    37,    78,\n",
       "           97,  2436,    67,  3950,    45,     2,    24,   105,   256,\n",
       "            1,   134,  1571,     2, 12399,   451,    14,   319,    11,\n",
       "           63,     6,    98,  1321,     5,   105,     1,  3767,     4,\n",
       "            3],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,    11,     6,\n",
       "           24,     1,   779,  3687,  2818,    20,     8,    14,    74,\n",
       "          325,  2730,    73,    90,     4,    27,    99,     2,   165,\n",
       "           68],\n",
       "       [   54,    10,    14,   116,    60,   798,   552,    71,   364,\n",
       "            5,     1,   730,     5,    66,  8057,     8,    14,    30,\n",
       "            4,   109,    99,    10,   293,    17,    60,   798,    19,\n",
       "           11,    14,     1,    64,    30,    69,  2500,    45,     4,\n",
       "          234,    93,    10,    68,   114,   108,  8057,   363,    43,\n",
       "         1009,     2,    10,    97,    28,  1431,    45,     1,   357,\n",
       "            4,    60,   110,   205,     8,    48,     3,  1929, 10880,\n",
       "            2,  2124,   354,   412,     4,    13,  6609,     2,  2974,\n",
       "         5148,  2125,  1366,     6,    30,     4,    60,   502,   876,\n",
       "           19,  8057,     6,    34,   227,     1,   247,   412,     4,\n",
       "          582,     4,    27,   599,     9,     1, 13586,   396,     4,\n",
       "        14047]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练、验证、测试数据集划分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当把数据处理为网络所需要的shape后，就需要将数据集划分为训练集、验证集、测试数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们定义一个划分系数，split_frac，代表数据保留到训练集中的比例，通常设置为0.8或0.9，然后剩余的数据评分为验证集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFeatures Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, val_x = train_test_split(features, test_size = 1 - split_frac, random_state = 0)\n",
    "train_y, val_y = train_test_split(labels, test_size = 1 - split_frac, random_state = 0)\n",
    "\n",
    "val_x, test_x = train_test_split(val_x, test_size = 0.5, random_state = 0)\n",
    "val_y, test_y = train_test_split(val_y, test_size = 0.5, random_state = 0)\n",
    "\n",
    "print(\"\\t\\tFeatures Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "    \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "    \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25001\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理完成之后，我们将构建图。第一步是定义好超参数：</br>\n",
    "- lstm_size：LSTM细胞隐藏单元数量，稍微设置大点会有不错的效果，常见的值如128, 256, 512等。\n",
    "- lstm_layers：网络中LSTM层的数量，这里从1开始，如果不合适就再增加。\n",
    "- batch_size：在一次训练中进入网络的数据量。通常情况下，应该设置大一些，如果你能确保内存足够的话。\n",
    "- learning_rate：学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于网络来说，它的输入是200个单词长度的组成的评论向量，每次batch的大小是预设的batch_size个向量。我们会在LSTM层添加dropout，因此会为每个单元被保留的概率提供占位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int) + 1\n",
    "#加1是因为字典从1开始，我们用0来填充\n",
    "\n",
    "#创建图对象\n",
    "graph = tf.Graph()\n",
    "\n",
    "#像图中添加节点\n",
    "with graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = 'inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name = 'labels')\n",
    "    keep_prod = tf.placeholder(tf.float32, name = 'keep_prod')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来添加一个嵌入层。需要这样做的原因是：在我们的词典里有74000个单词，如果使用One-Hot编码来处理将会是非常低效的。为了代替one-hot，我们使用一个嵌入层来作为一个查找表，我们可以使用一个word2vec训练的嵌入层模型，然后在这里加载使用。不过新建一个图并让网络学习权重也是可以的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码中使用tf.Variable来创建一个嵌入查找矩形，并使用它来使嵌入的向量通过tf.nn.embedding_lookup嵌入查找传递到LSTM单元。这个函数需要两个参数：嵌入矩阵和输入张量，比如一个评论向量。然后它会返回一个带有内嵌向量的张量。因此，如果嵌入层有200个单元，这个函数返回的大小为batch_size, 200]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#嵌入向量的大小(嵌入层单元个数)\n",
    "embed_size = 300\n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM细胞层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "</br>\n",
    "接下来我们将创建LSTM层用来构建RNN网络。需要注意的是这里并不是真正的构建图，而仅仅是\n",
    "定义好我们在图中需要的cell的类型。\n",
    "</br>\n",
    "我们将使用```tf.contrib.rnn.BasicLSTMCell```来在图中创建LSTM细胞层，，\n",
    "该方法的说明文档如下：\n",
    "\n",
    "```\n",
    "tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0, input_size = None,\n",
    "state_is_tuple = True, activation = <function tanh at 0x109flef28>)\n",
    "```\n",
    "</br>\n",
    "其中，num_units是细胞中单元数量，也就是lstm_size。因此，可以写成\n",
    "</br>\n",
    "```lstm = tf.contrib.rnn.BasicLSTMCell(num_units)```\n",
    "</br>\n",
    "然后，可以使用```tf.contrib.rnn.DropoutWrapper```来添加dropout。像这样子：\n",
    "\n",
    "```\n",
    "drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prod = keep_prod)\n",
    "\n",
    "```\n",
    "\n",
    "大多数情况下，越多的层数会使网络效果更好。这便是深度学习的神奇之处，添加更多的网络层能使得网络可以学习到更多复杂的东西。< /br>\n",
    "此外，还有一个用来创建多层LSTM单元的方式：```tf.contrib.rnn.MultiRNNCell```：\n",
    "\n",
    "```\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "\n",
    "```\n",
    "解释：```[drop] *```创建了一个长度为lstm_layers的cell列表(drop) ，MultiRNNCell包装器将其构建到RNN的多个层中，其中每个cell为列表中的每个cell。\n",
    "所以你在网络中实际使用的cell其实是有着dropout的多个(或者只有一个)LSTM cell。但是\n",
    "但从体系结构的角度来看，这一切都是一样的，只是单元格中的一个更复杂的图形。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的代码中，我们将使用tf.contrib.rnn.BasicLSTMCell去创建LSTM层。然后使用tf.contrib.rnn.DropoutWrapper添加dropout。最后使用tf.contrib.MultiRNNCell创建多个LSTM层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prod)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "现在我们需要将数据流入RNN节点中，可以使用```tf.nn.dynamic_rnn```来完成。\n",
    "我们需要传入前面创建的RNN(或者多层的LSTM cell以及网络的输入)\n",
    "```\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, \n",
    "initial_state=initial_state)\n",
    "\n",
    "```\n",
    "我们创建了一个初始状态initail_state来传入RNN。这是在连续时间步骤中在隐藏层之间传递的cell状态。```tf.nn.dynamic_rnn```做了大部分事情。我们传入cell以及细胞输入，它会处理额外的工作，然后返回每个时间步骤的输出以及最终状态。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们之只关心最终的输出结果，并用来作为情绪预测结果。我们用```outputs[:, -1]来获取最后的输出，并计算与labels的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据bacth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batchs(x, y, batch_size = 100):\n",
    "    n_batchs = len(x) // batch_size\n",
    "    x, y = x[:n_batchs * batch_size], y[:n_batchs * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batchs(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs : x,\n",
    "                   labels : y[:, None],\n",
    "                   keep_prod : 0.5,\n",
    "                   initial_state : state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict = feed)\n",
    "            \n",
    "        if iteration % 5 == 0:\n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "        if iteration % 25 == 0:\n",
    "            val_acc = []\n",
    "            val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for x, y in get_batchs(val_x, val_y, batch_size):\n",
    "                feed = {\n",
    "                    inputs : x,\n",
    "                    labels : y[:, None],\n",
    "                    keep_prod : 1,\n",
    "                    initial_state : val_state\n",
    "                }\n",
    "                batch_acc, val_state = sess.run([accuracy, final_state], feed_dict = feed)\n",
    "                val_acc.append(batch_acc)\n",
    "            print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "        iteration += 1\n",
    "    saver.save(sess, 'checkpoints/sentiment.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    saver.restore(sess, tf.train.lastest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        fedd = {inputs : x,\n",
    "               labels : y[:, None],\n",
    "               keep_prod : 1,\n",
    "               initial_state = test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict = feed)\n",
    "        test.acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
